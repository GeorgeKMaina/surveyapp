{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7d0078-ab7a-41ac-9b3d-14aa46045704",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'text_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# ✅ Import the clean_text function from external module\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_text\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(filepath: \u001b[38;5;28mstr\u001b[39m, question_col: \u001b[38;5;28mstr\u001b[39m, label_col: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    Load the data from an Excel file, clean the questions, and check for missing labels.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'text_utils'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ✅ Import the clean_text function from external module\n",
    "from text_utils import clean_text\n",
    "\n",
    "\n",
    "def load_data(filepath: str, question_col: str, label_col: str):\n",
    "    \"\"\"\n",
    "    Load the data from an Excel file, clean the questions, and check for missing labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(filepath)\n",
    "        if question_col not in df.columns or label_col not in df.columns:\n",
    "            raise ValueError(f\"Columns '{question_col}' and '{label_col}' must be present.\")\n",
    "        df[question_col] = df[question_col].astype(str).apply(clean_text)\n",
    "        if df[label_col].isnull().any():\n",
    "            missing_count = df[label_col].isnull().sum()\n",
    "            logging.warning(\"There are %d missing labels.\", missing_count)\n",
    "        else:\n",
    "            logging.info(\"All questions have labels.\")\n",
    "        logging.info(\"Sample cleaned questions:\\n%s\", df[question_col].head().to_string())\n",
    "        return df[question_col], df[label_col]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error reading the Excel file: %s\", e)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def create_pipeline(model):\n",
    "    \"\"\"\n",
    "    Create a pipeline that includes TF-IDF vectorization (with cleaning) and the classifier.\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english', preprocessor=clean_text)),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate_model(pipeline, X, y, cv=5) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a pipeline using cross-validation and return the mean accuracy.\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Configuration\n",
    "    filepath = r\"C:\\Users\\Wangari Kimani\\Downloads\\sample questions.xlsx\"\n",
    "    question_col = 'Question'\n",
    "    label_col = 'Question Type'\n",
    "\n",
    "    logging.info(\"Loading data from %s\", filepath)\n",
    "    X, y = load_data(filepath, question_col, label_col)\n",
    "    print(\"Data loaded successfully!\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Candidate models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'SVM': LinearSVC(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "        'Naive Bayes': MultinomialNB()\n",
    "    }\n",
    "\n",
    "    base_results = {}\n",
    "    logging.info(\"Starting base model evaluation using cross-validation.\")\n",
    "    for name, model in models.items():\n",
    "        logging.info(\"Evaluating base model: %s\", name)\n",
    "        pipeline = create_pipeline(model)\n",
    "        score = evaluate_model(pipeline, X_train, y_train)\n",
    "        base_results[name] = score\n",
    "        logging.info(\"Base %s Cross-Validation Accuracy: %.4f\", name, score)\n",
    "\n",
    "    print(\"Base model evaluation results:\")\n",
    "    for model_name, accuracy in base_results.items():\n",
    "        print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "    # Best model selection\n",
    "    best_model_name = max(base_results, key=base_results.get)\n",
    "    best_model = models[best_model_name]\n",
    "    print(\"Best base model:\", best_model_name)\n",
    "\n",
    "    # Hyperparameter grid\n",
    "    if best_model_name == 'Logistic Regression':\n",
    "        param_grid = {'clf__C': [0.1, 1, 10]}\n",
    "    elif best_model_name == 'SVM':\n",
    "        param_grid = {'clf__C': [0.1, 1, 10]}\n",
    "    elif best_model_name == 'Random Forest':\n",
    "        param_grid = {'clf__n_estimators': [50, 100, 200]}\n",
    "    elif best_model_name == 'Naive Bayes':\n",
    "        param_grid = {'clf__alpha': [0.5, 1.0, 1.5]}\n",
    "    else:\n",
    "        param_grid = {}\n",
    "\n",
    "    pipeline = create_pipeline(best_model)\n",
    "\n",
    "    if param_grid:\n",
    "        logging.info(\"Starting hyperparameter tuning for %s using GridSearchCV.\", best_model_name)\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        tuned_score = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "        logging.info(\"Best hyperparameters for %s: %s\", best_model_name, best_params)\n",
    "        print(\"Best hyperparameters:\", best_params)\n",
    "        print(f\"CV Accuracy after tuning: {tuned_score:.4f}\")\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "    else:\n",
    "        best_pipeline = pipeline\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Final test set evaluation\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save the final model\n",
    "    model_path = 'best_model.pkl'\n",
    "    joblib.dump(best_pipeline, model_path)\n",
    "    logging.info(\"Best model saved to %s\", os.path.abspath(model_path))\n",
    "    print(\"Best model saved to\", os.path.abspath(model_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
